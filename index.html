<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiT-VTON: Exploring Diffusion Transformer Framework for Multi-Category Virtual Try-On with Integrated Image Customization.">
  <meta name="keywords" content="Virtual Try-On, Diffusion Transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiT-VTON: Exploring Diffusion Transformer Framework for Multi-Category Virtual Try-On with Integrated Image Customization</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Virtual Try-On Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://deft-vton.github.io/">
            Efficient Virtual Try-On with Consistent Generalised H-Transform
          </a>
          <a class="navbar-item" href="https://pose-vton.github.io/vto-pose-conditioning/">
            Is concatenation really all you need for pose-conditioning and pose control
          </a>
          <a class="navbar-item" href="https://instructvton.github.io/instruct-vton.github.io/">
            InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Qi Li</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="">Shuwen Qiu</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="">Julien Han</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Kee Kiat Koo</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Mehmet Saygin Seyfioglu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Karim Bouyarmane</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-bottom: 1px;">
            <span class="author-block"><sup>1</sup>Amazon</a>&nbsp   </a>&nbsp  </span>
            <br>
            <span class="author-block"><sup>2</sup>
                <span style="font-size: 0.7em;">
                  University of California, Los Angeles. Work done during internship at Amazon &nbsp;&nbsp; <sup>*</sup> Equal Contribution
                </span></a>&nbsp</a>&nbsp  
              </span>
          </div>
          <!-- <div class="is-size-6 has-text-centered" style="margin-top: 1px;">
            <sup>*</sup> Equal Contribution
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="./static/videos/end_to_end.m4v" 
                   class="external-link button is-normal is-rounded is-dark"
                   download>
                    <span class="icon">
                        <i class="fas fa-download"></i>
                    </span>
                    <span>Video</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <div class="image-area">
            <img src="./static/images/intro.png" alt="pipeline" width="100%">
          </div>
          <div class="content has-text-justified">
            <p>
              The DiT-VTON model offers diverse use cases, enabling inpainting within user-specified editing regions using content guided by a reference image. 
              The model can semantically infer and generate expected objects, textures, perform local editing, and even identify specific body parts for virtual try-on tasks including multi-garment try-on, showcasing its 
              versatility in content-aware editing and synthesis. Meanwhile, we have pioneered the expansion of this research area beyond traditional garment virtual try-on to to virtual try-all, extending its application 
              to a wide range of product categories, including furniture, jewelry, shoes, and other wearables such as scarves, glasses, and handbags, etc.
            </p>
          </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Zero Video -->
        <div class="item item-chair-tp">
          <video poster="" id="in-the-wild" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/in_the_wild.m4v" type="video/mp4">
          </video>
          <br>
          <h2 class="subtitle">
            <span class="subvideo"></span> Vritual Try-On Examples
          </h2>
          <br>
        </div>

        <!-- First Video -->
        <div class="item item-chair-tp">
            <video poster="" id="multi-garment" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/multi_garment.m4v" type="video/mp4">
            </video>
            <br>
            <h2 class="subtitle has-text-centered">
              <span class="subvideo"></span> Multi-Garments, and Mask Control
            </h2>
            <br>
        </div>

        <!-- Second Video -->
        <div class="item item-shiba">
            <video poster="" id="vit-all" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/vit_all.m4v" type="video/mp4">
            </video>
            <br>
            <h2 class="subtitle has-text-centered">
              <span class="subvideo"></span> Virtual Try-All for Non-Garment categories
            </h2>
            <br>
        </div>

        <!-- Third Video -->
        <div class="item item-blueshirt">
            <video poster="" id="style_transfer" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/style_transfer.m4v" type="video/mp4">
            </video>
            <br>
            <h2 class="subtitle has-text-centered">
              <span class="subvideo"></span> Texture Transfer
            </h2>
            <br>
        </div>

        <!-- Fourth Video -->
        <div class="item item-mask">
            <video poster="" id="local_editing" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/local_editing.m4v" type="video/mp4">
            </video>
            <br>
            <h2 class="subtitle has-text-centered">
              <span class="subvideo"></span> Localized region editing and refinement
            </h2>
            <br>
        </div>

          <!-- Fifth Video -->
          <!-- <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/vit_all.mp4" type="video/mp4">
            </video>
            <br>
            <h2 class="subtitle has-text-centered">
              <span class="subvideo"></span> Virtual Try-All for Non-Garment categories
            </h2>
            <br>
          </div> -->
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid growth of e-commerce has intensified the demand
            for Virtual Try-On (VTO) technologies, enabling customers
            to realistically visualize products overlaid on their own images. Despite advances, existing VTO models face challenges with fine-grained detail preservation, robustness to
            real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product types.
            In this paper, we present DiT-VTON, a novel VTO framework that leverages the Diffusion Transformer (DiT)-based
            text-to-image (T2I) model, renowned for high-fidelity image generation, adapted here for the image-conditioned
            VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine
            the optimal setup for VTO. Our findings indicate that token concatenation combined with pose stitching yields the
            best performance. To enhance robustness, we train the
            model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO
            adaptability. For the first time, DiT-VTON redefines the
            VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide
            range of product types and supporting advanced image
            editing functionalities, such as pose preservation, precise
            localized region editing and refinement, texture transfer
            and object-level customization. Experimental results show
            that our model surpasses state-of-the-art methods on public datasets VITON-HD and DressCode, achieving superior
            detail preservation and robustness without reliance on external condition encoders. Additionally, DiT-VTON leverages rectified flow for faster inference and can be adapted
            to any pretrained T2I model across datasets of varying resolutions. This work significantly advances VTO applicability
            in diverse real-world scenarios, enhancing both the realism
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
  <!-- Key Contributions. -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Key Contributions</h2>
      <div class="content has-text-justified">
        <ul>
          <li><b>Virtual Try-All (VTA) Capability:</b> Expanding virtual try-on beyond garments to a wide range of product categories, including furniture, jewelry, shoes, and accessories, etc.</li>
            <!-- <div class="image-area" style="margin: 15px 0;">
              <img src="./static/images/vit_all.png" alt="More VTA Examples" width="100%" height="100%">
            </div> -->
          <li><b>Diffusion Transformer-based VTO:</b> Leveraging DiT for high-fidelity, image-conditioned virtual try-on with superior detail preservation.</li>
            <!-- <div class="image-area" style="margin: 15px 0;">
              <img src="./static/images/more_vto_example.png" alt="More VTO Examples" width="100%" height="100%">
            </div> -->
          <li><b>Multi-Garment Try-On:</b> Allows seamless try-on of multiple garments simultaneously while maintaining realistic composition.</li>
          <li><b>Local Editing:</b> Enables precise image customization, including logo refinement and pattern modification, for enhanced personalization.</li>
            <!-- <div class="image-area" style="margin: 15px 0;">
              <img src="./static/images/image_customization.png" alt="Image Customization Example" width="100%" height="100%">
            </div> -->
          <li><b>Texture and Style Transfer:</b> Accurately transfers styles and textures from reference images, enabling a more flexible and realistic try-on experience.</li>
          <li><b>Advanced Pose Control and Pose-Oriented Generation:</b> Supports pose-preserving transformations and pose-guided image generation without adding extra model components or parameters. Find more details in our research paper <a href="https://pose-vton.github.io/vto-pose-conditioning/" target="_blank">"Is Concatenation Really All You Need: Efficient Concatenation-Based Pose Conditioning and Pose Control for Virtual Try-On"</a>.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Re-rendering. -->
  <div class="content has-text-centered">
    <img src="./static/videos/pose_control.gif" alt="Pose Control" width="45%" height="45%">
  </div>
  <!--/ Re-rendering. -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="text-align: center;">Overall Pipeline</h2>
        <div class="image-area">
          <img src="./static/images/model2_arxiv.png" alt="Overall Pipeline" width="100%">
        </div>
        <div class="content has-text-centered">
          <p>
            Illustration of different model configurations of DiT-VTON to effectively integrate image conditions.
          </p>
        </div>
        <div class="content has-text-justified">
          <br>
             We explore the optimal model configuration to integrate
            additional image conditions into the transformer blocks. <b>(Left)</b> Channel concatenation. We follow convention in UNet-based inpainting models that concatenates the masked image
             I<sub>e</sub>, the mask image I<sub>m</sub> and the latent noise x<sub>t</sub> in the channel dimension. As for the additional reference image I<sub>r</sub>, we concatenate it with the masked image at 
             the spatial dimension. <b>(Middle)</b> ControlNet. Adding control to diffusion models by copy (part of) the main denoising backbone as the ControlNet to encode conditions. Then the encoded image representation
              is fused back to the main backbone by cross attention, summation or other adaptive norm layers. <b>(Right)</b> Token Concatenation. we patchify each latent image into tokens and directly concatenate all the 
              image tokens together as the input.
          </p>
        </div>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing,
  author    = {Shuwen Qiu, Qi Li, Amir Tavanaei, Julien Han, Kee Kiat Koo, Karim Bouyarmane},
  title     = {DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is modified based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
